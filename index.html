<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?">
  <meta name="keywords" content="MATP-BENCH, Benchmark, Automated Theorem Prover, Multimodal Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/MATP_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script>hljs.highlightAll();</script>
  <script type="module">
    import { Client } from "https://cdn.skypack.dev/@gradio/client";
    window.Client = Client;
  </script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- æ·»åŠ å›¾ç‰‡ --> 
          <img src="./static/images/MATP_icon.png" alt="Logo" style="max-width: 280px; margin-bottom: 40px;">
          <h1 class="title is-1 publication-title">MATP-BENCH: Can MLLM Be a Good<br>Automated Theorem Prover for <br>Multimodal Problems?</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>
              <a href="https://scholar.google.com/citations?user=ULvoYXgAAAAJ&hl=zh-CN">Zhitao He</a>,
            </span>
            <span class="author-block"><sup>1</sup>
              <a href="https://matpbench.github.io/">Zongwei Lyu</a>,
            </span>
            <span class="author-block"><sup>2</sup>
              <a href="https://matpbench.github.io/">Dazhong Chen</a>,
            </span>
            <span class="author-block"><sup>1</sup>
              <a href="https://scholar.google.com/citations?user=5yHRd6kAAAAJ&hl=zh-CN&oi=ao">Dadi Guo</a>,
            </span>
            <span class="author-block"><sup>1</sup>
              <a href="https://mayrfung.github.io/">Yi R. (May) Fung</a>
            </span>
            <!-- <span class="author-block">
              <a href="https://aclanthology.org/people/j/jiachun-li/">Jiachun Li</a>,
            </span> -->
            <!-- <span class="author-block">
              <a href="https://nlpr.ia.ac.cn/cip/yubochen/index.html">Yubo Chen</a>,
            </span> -->
            <!-- <span class="author-block">
              <a href="https://nlpr.ia.ac.cn/cip/~liukang/index.html">Kang Liu</a>,
            </span>
            <span class="author-block">
              <a href="https://nlpr-web.ia.ac.cn/cip/english/~junzhao/index.html">Jun Zhao</a>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> The Hong Kong University of Science and Technology,</span>
            <br>
            <span class="author-block"><sup>2</sup> Chinese University of Hong Kong (Shenzhen)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/MATP-Bench.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://matpbench.github.io/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Zhitao-He/MATPBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Zhitao-He/MATPBench"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      ðŸ¤—
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Summary -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <p>
            Numerous theorems, such as those in geometry, are often presented in multimodal forms (e.g., diagrams). 
            Humans benefit from visual reasoning in such settings, using diagrams to gain intuition and guide the proof process. 
            Modern Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in solving a wide range of mathematical problems. 
            However, the potential of MLLMs as Automated Theorem Provers (ATPs), specifically in the multimodal domain, remains underexplored. 
            In this paper, we introduce the <span style="color: red;">M</span>ultimodal <span style="color: red;">A</span>utomated <span style="color: red;">T</span>heorem <span style="color: red;">P</span>roving Benchmark (<span style="color: red;">MATP-Bench</span>), a new Multimodal, Multi-level, and Multi-language benchmark 
            designed to evaluate MLLMs in this role as multimodal automated theorem provers. MATP-BENCH consists of <span style="color: red;">1056 multimodal theorems</span> drawn from <span style="color: red;">high school, university, and competition-level</span> mathematics. 
            All these multimodal problems are accompanied by <span style="color: red;">formalizations in Lean 4, Coq and Isabelle</span>, thus making the benchmark compatible with a wide range of theorem-proving frameworks. 
            MATP-BENCH requires models to integrate sophisticated visual understanding with mastery of a broad spectrum of mathematical knowledge and rigorous symbolic reasoning to generate formal proofs. 
            We use MATP-BENCH to evaluate a variety of advanced multimodal language models. Existing methods can only solve a limited number of the MATP-BENCH problems, indicating that this benchmark poses an open challenge for research on automated theorem proving. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  <!-- Paper image. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Multimodal Automated Theorem Proving (MATP)</h2>
      <div class="box py-2" style="height: 100%; display: flex; align-items: center; justify-content: center;">
        <div class="publication-image"> 
        <img src="./static/images/fig1.png" alt="Description of the image">
        <figcaption class="has-text-centered mt-2"> Figure 1. We illustrate the differences between traditional <b>ATP and MATP</b> through examples from miniF2F (above) and MATPBench (below).  </figcaption>
      </div>
    </div>
    </div>
  </div>
</br>
</br>
</br>
  <!--/ Paper image. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <!-- <h3 class="title is-4">Memorization Quantification</h3> -->
      <div class="content has-text-justified">
        <p>
          Multimodal theorems consist of an image paired with a natural language theorem statement, which complement each other to convey complete theorem information. 
          Furthermore, additional auxiliary constructions are often essential for their proof (as shown in the bottom left subfigure). In traditional ATP, 
          theorem formalization relies solely on textual statements (we use <span style="color: purple;">purple</span> to indicate premises derived from the original statement), 
          whereas MATP requires the model to extract critical premises not explicitly expressed in the text by analyzing accompanying diagrams (see <b>From diagram</b> on the right). 
          We provide formalized versions of all multimodal theorems in Lean4, Coq, and Isabelle.
        </p>
      </div>
      <div class="column is-full-width">
        <h3 class="title is-4">Problem Formulation</h3>
        <div class="content has-text-justified">
          <p>
            <span style="color: red; font-weight: bold;">Automated Theorem Proving (ATP)</span>: In this task, the system takes a <span style="color: purple; font-weight: bold;">formalized theorem statement</span> <em>T</em> as input. The goal is to generate a formal proof <em>P</em> such that:
          </p>
          
          <div style="text-align: center; margin: 1.5em 0; font-family: 'Times New Roman', serif; font-size: 1.2em; line-height: 1.8;">
            Prover<sub style="font-size: 0.8em;">ATP</sub>(<em>T</em>) â†’ <em>P</em>,<br>
            where Check(<em>T</em>, <em>P</em>) = True.
          </div>
          
          <p>
            Here, <strong>Check</strong> denotes the built-in proof verifier of the formal system (such as Lean, Coq, or Isabelle), which ensures that the generated proof <em>P</em> is a valid derivation of theorem <em>T</em>.
          </p>
      
          <p>
            <span style="color: red; font-weight: bold;">Multimodal Automated Theorem Proving (MATP)</span>: In this task, the input to the MATP system is a pair (<em>I</em>, <em>S</em>), where:
          </p>
          
          <ul>
            <li><em>I</em> is a <span style="color: purple; font-weight: bold;">multimodal input</span> (e.g., geometric figure);</li>
            <li><em>S</em> is a <span style="color: purple; font-weight: bold;">natural language statement of the theorem</span> (not formalized).</li>
          </ul>
          
          <p>
            As shown in Figure 1, the natural language statement <em>S</em> and the information from the multimodal input <em>I</em> are complementary, together forming a complete theorem. Hence, the model must first generate the complete formal theorem <em>T</em>, then generate a valid formal proof <em>P</em>. The entire MATP task can be summarized as:
          </p>
          
          <div style="text-align: center; margin: 1.5em 0; font-family: 'Times New Roman', serif; font-size: 1.2em; line-height: 1.8;">
            Prover<sub style="font-size: 0.8em;">MATP</sub>(<em>I</em>, <em>S</em>) â†’ (<em>T</em>, <em>P</em>),<br>
            where Check(<em>T</em>, <em>P</em>) = True.
          </div>
      
          <p>
            To avoid <strong>modality leakage</strong>, where the model could ignore visual inputs, we provide only natural language <em>S</em> and image <em>I</em>, without the formalized theorem <em>T</em>. This encourages the model to interpret and reason over multimodal inputs, as humans do when proving multimodal theorems. Furthermore, we incorporate a <strong>theorem verification</strong> task in the experimental setup to ensure that the formal theorems automatically generated by the multimodal model are consistent with the problem statement, rather than fabricating simple theorems arbitrarily.
          </p>
        </div>
      </div>
      <!-- <div class="publication-image">
        <img src="./static/images/Mem.jpg" alt="Description of the image">
      </div> -->

      <br/>
    </div>
  </div>

  <!-- Paper image. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width">
      <h2 class="title is-3">Benchmark Comparison</h2>
      <div class="content has-text-justified">
        <p>
          We provide a detailed comparison between existing variable math reasoning benchmarks and MATP-Bench.
        </p>
      </div>
      <div class="box py-2" style="height: 100%; display: flex; align-items: center; justify-content: center;">
      <div class="publication-image">
        <img src="./static/images/tab1.png" alt="Description of the image">
        <figcaption class="has-text-centered mt-2"> Figure 2. Comparison of existing related benchmarks. MATP-BENCH is a Multimodal, Multi-level,
          and Multi-language benchmark designed to evaluate MLLMs as automated theorem provers. </figcaption>
      </div>
    </div>
    </div>
  </div> 
  <!--/ Paper image. -->
</section>
</br>
</br>
</br>
<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">MATP-BENCH Features</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4">Multimodal Context and Multi-language Theorem</h3>
        <div class="content has-text-justified">
          <p>
            As shown in the lower part of Figure 1, each theorem consists of an image and a corresponding natural language description, which complement each other to form a complete statement. MATP-BENCH provides formalizations of these multimodal theorems in Lean 4, Isabelle, and Coq. To the best of our knowledge, MATP-BENCH is the first multimodal automated theorem proving benchmark covering all three of these languages.
          </p>
        </div>
        <h3 class="title is-4">Hierarchy and Diversity</h3>
        <div class="content has-text-justified">
          <p>
            The problems in MATP-BENCH span three distinct educational stagesâ€”high school, university, and competitionsâ€”systematically covering a wide range of difficulty levels from elementary to advanced. Specifically, the high school and university problems are collected from publicly available multimodal math problem datasets, while the competition problems are sourced from public Mathematical Olympiad examinations. Moreover, the multimodal theorems in MATP-BENCH are primarily centered around the domain of geometry, spanning plane geometry, 3D geometry, analytic geometry.
          </p>
        </div>
      </div>
      <div class="column is-full-width">
        <h3 class="title is-4">Task Formulation</h3>
        <div class="content has-text-justified">
          <p>
            We aim to achieve end-to-end multimodal automated theorem proving (Task 1), where the input is a natural language theorem and an image, and the output is a formal theorem and its proof, i.e. <span style="font-family: 'Times New Roman', serif;">Prover<sub>MATP</sub>(<em>I</em>, <em>S</em>) â†’ (<em>T</em>, <em>P</em>)</span>. Furthermore, to prevent the model from generating formal theorems that do not align with original problems, we separately set up multimodal theorem formalization (Task 2) for verification, which aligns with LeanEuclid. Thus, we divide the task into two progressively challenging sub-tasks:
          </p>
          
          <ul style="margin-left: 1.5em;">
            <li style="margin-bottom: 0.5em;">
              <strong>Task 1: Multimodal Automated Theorem Proving</strong>:<br>
              This task aims to achieve end-to-end multimodal automated theorem proving similar to human provers, by directly generating a formalized theorem <em>T</em> and its proof <em>P</em> from multimodal informal input, i.e. <span style="font-family: 'Times New Roman', serif;">Prover<sub>Task1</sub>(<em>I</em>, <em>S</em>) â†’ (<em>T</em>, <em>P</em>)</span>.
            </li>
            <li>
              <strong>Task 2: Multimodal Theorem Formalization</strong>:<br>
              The prover receives the multimodal question, and is required to formalize it into a precise theorem <em>T</em>, formally denoted as <span style="font-family: 'Times New Roman', serif;">Prover<sub>Task2</sub>(<em>I</em>, <em>S</em>) â†’ <em>T</em></span>. This task evaluates the model's ability to correctly understand and formalize information from both textual and visual modalities.
            </li>
          </ul>
        </div>
      </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">MATP-BENCH Features</h2>
    <div class="columns is-multiline">
      <!-- Top Left -->
      <div class="column is-half">
        <h3 class="title is-4">Multimodal Context and Multi-language Theorem</h3>
        <div class="content has-text-justified">
          <p>
            MATP-BENCH introduces concrete multimodal contexts to jointly evaluate models on visual understanding, mathematical reasoning, and symbolic manipulation. As shown in the lower part of Figure 1, each theorem consists of an image and a corresponding natural language description, which complement each other to form a complete statement. MATP-BENCH provides formalizations of these multimodal theorems in Lean 4, Isabelle, and Coq. To the best of our knowledge, MATP-BENCH is the first multimodal automated theorem proving benchmark covering all three of these languages.
          </p>
        </div>
      </div>
      
      <!-- Top Right -->
      <div class="column is-half">
        <h3 class="title is-4">Hierarchy and Diversity</h3>
        <div class="content has-text-justified">
          <p>
            The problems in MATP-BENCH span three distinct educational stagesâ€”high school, university, and competitionsâ€”systematically covering a wide range of difficulty levels from elementary to advanced. Specifically, the high school and university problems are collected from publicly available multimodal math problem datasets, while the competition problems are sourced from public Mathematical Olympiad examinations. Moreover, the multimodal theorems in MATP-BENCH are primarily centered around the domain of geometry, spanning plane geometry, 3D geometry, analytic geometry.
          </p>
        </div>
      </div>
      
      <!-- Bottom Left -->
      <div class="column is-half">
        <h3 class="title is-4">Task Formulation</h3>
        <div class="content has-text-justified">
          <p>
            We aim to achieve end-to-end multimodal automated theorem proving (Task 1), where the input is a natural language theorem and an image, and the output is a formal theorem and its proof, i.e. <span style="font-family: 'Times New Roman', serif;">Prover<sub>MATP</sub>(<em>I</em>, <em>S</em>) â†’ (<em>T</em>, <em>P</em>)</span>. Furthermore, to prevent the model from generating formal theorems that do not align with original problems, we separately set up multimodal theorem formalization (Task 2) for verification, which aligns with LeanEuclid. Thus, we divide the task into two progressively challenging sub-tasks:
          </p>
          
          <ul style="margin-left: 1.5em;">
            <li style="margin-bottom: 0.5em;">
              <strong><span style="color: red;">Task 1: Multimodal Automated Theorem Proving</span></strong><br>
              This task aims to achieve end-to-end multimodal automated theorem proving similar to human provers, by directly generating a formalized theorem <em>T</em> and its proof <em>P</em> from multimodal informal input, i.e. <span style="font-family: 'Times New Roman', serif;">Prover<sub>Task1</sub>(<em>I</em>, <em>S</em>) â†’ (<em>T</em>, <em>P</em>)</span>.
            </li>
            <li>
              <strong><span style="color: red;">Task 2: Multimodal Theorem Formalization</span></strong><br>
              The prover receives the multimodal question, and is required to formalize it into a precise theorem <em>T</em>, formally denoted as <span style="font-family: 'Times New Roman', serif;">Prover<sub>Task2</sub>(<em>I</em>, <em>S</em>) â†’ <em>T</em></span>. This task evaluates the model's ability to correctly understand and formalize information from both textual and visual modalities.
            </li>
          </ul>
        </div>
      </div>
      
      <!-- Bottom Right - Image -->
      <div class="column is-half">
        <div class="box" style="height: 100%; display: flex; align-items: center; justify-content: center;">
          <!-- Replace with your actual image -->
          <figure class="image">
            <img src="./static/images/tab2.png" alt="MATP-BENCH illustration">
            <figcaption class="has-text-centered mt-2"> Figure 3. Statistics summary of MATP-BENCH. Counts and
              percentages are provided for each category </figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <div class="columns is-centered has-text-centered">
  <iframe src="" frameborder="0" width="75%" height="800"></iframe>
</div>
 -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-3">Experimental Results</h3>
        <div class="content has-text-justified">
          <p>
            Experimental results of <b>Multimodal Automated Theorem Proving (Task 1)</b>, which requires
model to generate both formalized theorem and proof. We adopt pass@10 as the evaluation metric.
We further present the experimental results of pass@n (n=1, n=5) in our paper (Appendix).
          </p>
        </div>
        <div class="content has-text-centered">
        <div class="box" style="height: 100%; display: flex; align-items: center; justify-content: center;">
          <div class="publication-image">
            <img src="./static/images/tab3.png" alt="Description of the image">
          </div>
        </div>
      </div>
        <div class="content has-text-justified">
          <p>
            Experimental results of <b>Multimodal Theorem Formalization (Task 2)</b>, which only require
model to generate formalized theorem. We use GPT-4o as the judge and adopt pass@10 as the
evaluation metric. We present the experimental results of pass@n (n=1, n=5) in our paper (Appendix).
          </p>
        </div>
        <div class="content has-text-centered">
        <div class="box" style="height: 100%; display: flex; align-items: center; justify-content: center;">
          <div class="publication-image">
            <img src="./static/images/tab4.png" alt="Description of the image">
          </div>
        </div>
      </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Analysis of Error Distribution</h2>

        <div class="content has-text-justified">
          <p>
            We analyze the types of errors generated by multimodal models in theorem proving. As shown in Figure, the results indicate that different models exhibit both common errors and specific issues. 
            This results of Claude and GPT-4.1 suggest that even for relatively better-performing models, the core challenges lie in complex logical reasoning and identifying and utilizing all implicit and explicit information required by the theorem. More fundamental formalization errors such as missing/incorrect library imports and incorrect/undeclared variables/definitions are more prominent in Qwen2.5. This might indicate that while Qwen struggles with proof step errors, it also faces significant issues in generating basic code that conforms to the formal language specification. Overall, all models exhibit problems with incomplete information understanding and broken reasoning chains.
          </p>
          
        </div>
        <!-- <div class="box py-2" style="height: 100%; display: flex; align-items: center; justify-content: center;"> -->
        <div class="publication-image">
          <img src="./static/images/fig2.png" alt="Description of the image">
        </div>
      <!-- </div> -->
    </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Main Bottleneck in Multimodal Automated Theorem Proving</h2>

        <div class="content has-text-justified">
          <p>
            the analysis of the pass@n performance of multimodal models in the Coq language shows that for both complete theorem formalization and proof generation (Task 1) and formalization only (Task 2), allowing models to generate more candidates (from pass@1 to pass@10) generally increases the success rate. However, the pass@n success rate for Task 1 is significantly lower than the pass@n success rate for Task 2 across all difficulty levels and models. This large performance gap between Task 1 and Task 2 is consistent across all pass@n settings, demonstrating that models have shown a certain ability in converting natural language descriptions and geometric figures into Coq formal statements (Task 2 pass@n is relatively high), but still face significant challenges in the subsequent complex logical reasoning and constructing formal proofs.
          </p>
          
        </div>
        
        <div class="publication-image">
          <img src="./static/images/fig3.png" alt="Description of the image">
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Capability in Generating Auxiliary Constructions for Multimodal Theorems</h2>
    
        <div class="content has-text-justified">
          <figure class="image is-pulled-right" style="width: 50%; margin-left: 2em;">
            <img src="./static/images/fig4.png" alt="Description of the image">
          </figure>
          
          <p>
            A characteristic distinguishing multimodal theorem proving from pure text theorem proving is that many theorems require the construction of auxiliary lines to aid thinking, especially problems at the competition level. Therefore, we further investigate the models' ability to construct auxiliary lines during the proof process.
            With the increase in theorem difficulty, the proportion of problems requiring auxiliary construction significantly rises, confirming the importance of auxiliary construction for solving high-difficulty geometric theorems. Claude 3.7 and GPT 4.1, which perform best in Task 1 and Task 2, attempt to perform auxiliary constructions when generating proofs, and this attempt rate also increases with difficulty, which indicates that the models possess a certain degree of autonomous auxiliary construction capability and awareness. However, the success rate of these proofs containing auxiliary constructions is very low. This prominently indicates that while models can introduce steps or concepts involving auxiliary constructions in proofs, they cannot effectively utilize these constructions to advance the proof process.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{he2025matpbench,
    title={MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?},
    author={Zhitao He, Zongwei Lyu, Dazhong Chen, Dadi Guo, Yi R. (May) Fung},
    year={2025},
    eprint={},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
      <!--       eprint={2402.18154}, -->

</b></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/Zhitao-He/MATPBench" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
